{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate HOG Feature from given input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py:248: skimage_deprecation: Argument `visualise` is deprecated and will be changed to `visualize` in v0.16\n",
      "  'be changed to `visualize` in v0.16', skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "ppc = 4\n",
    "hog_images = []\n",
    "hog_features = []\n",
    "for image in df['pixels']:\n",
    "    image = np.fromstring(image,dtype='int',sep=' ').reshape(48,48).astype('uint8')\n",
    "    fd,hog_image = hog(image, orientations=9, pixels_per_cell=(ppc,ppc),cells_per_block=(1, 1),block_norm= 'L2',visualise=True)\n",
    "    hog_images.append(hog_image)\n",
    "    hog_features.append(fd.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hog_feature'] = hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate training csv file\n",
    "df_train = df[df['Usage'] == 'Training']\n",
    "df_train.to_csv(\"output/train_hog.csv\")\n",
    "\n",
    "#Generate test csv file\n",
    "df_test = df[~df.isin(df_train).all(1)]\n",
    "df_test.to_csv(\"output/test_hog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate X_train, Y_train\n",
    "\n",
    "X_train = []\n",
    "train_hog = np.array(df_train['hog_feature'])\n",
    "for each in train_hog:\n",
    "    each = np.array(each)\n",
    "    X_train.append(each)\n",
    "\n",
    "#X_train\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "Y_train = np.array(df_train['emotion (label)'])\n",
    "#Y_train\n",
    "Y_train = Y_train.reshape(len(Y_train),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x_test, y_test\n",
    "\n",
    "x_test = []\n",
    "test_hog = np.array(df_test['hog_feature'])\n",
    "for each in test_hog:\n",
    "    each = np.array(each)\n",
    "    x_test.append(each)\n",
    "\n",
    "#X_train\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "y_test = np.array(df_test['emotion (label)'])\n",
    "#Y_train\n",
    "y_test = y_test.reshape(len(y_test),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initiate svm and train with training feature and label\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict label for test feature\n",
    "\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_test['Row Number'] = df_test.index\n",
    "\n",
    "df_test['pred_label'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df with columns predicted label and Row Number from input.csv(please note that indexing start from 0 and not 1)\n",
    "df_pred = df_test[['pred_label','Row Number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving csv of predicted labels\n",
    "\n",
    "df_pred.to_csv(\"output\\predicted_label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test data\n",
    "\n",
    "\n",
    "pred_scr = precision_score(y_test, y_pred)\n",
    "recall_scr = recall_score(y_test, y_pred)\n",
    "f1_scr = f1_score(y_test, y_pred)\n",
    "acr_score = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8018539976825029, 0.3900789177001127, 0.5248388320060675, 0.8254388409027584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89      5404\n",
      "           1       0.80      0.39      0.52      1774\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      7178\n",
      "   macro avg       0.82      0.68      0.71      7178\n",
      "weighted avg       0.82      0.83      0.80      7178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(F\"{ pred_scr }, { recall_scr }, { f1_scr }, { acr_score }\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/test_evaluation.txt\",\"a\") as file:\n",
    "    file.write(F\" Precision score is -> { pred_scr } \\n\")\n",
    "    file.write(F\" Recall score is -> { recall_scr } \\n\")\n",
    "    file.write(F\" F1 score is -> { f1_scr } \\n\")\n",
    "    file.write(F\" Acuuracy score is -> { acr_score } \\n\")\n",
    "    file.write(F\"Classification report is below \\n\\n\\n { classification_report(y_test, y_pred) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train evaluation\n",
    "\n",
    "#shuffle train_set and get 20 percentage of training data for evaluation, i am calling it as val\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, Y_train = shuffle(X_train, Y_train, random_state=15)\n",
    "\n",
    "X_val = X_train[:int(len(X_train) * 20 / 100 )]\n",
    "Y_val_true = Y_train[:int(len(Y_train) * 20 / 100 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scr_val = precision_score(Y_val_true, Y_val_pred)\n",
    "recall_scr_val = recall_score(Y_val_true, Y_val_pred)\n",
    "f1_scr_val = f1_score(Y_val_true, Y_val_pred)\n",
    "acr_score_val = accuracy_score(Y_val_true, Y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8310055865921788, 0.40669856459330145, 0.5461220743460303, 0.8277303605643616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89      4278\n",
      "           1       0.83      0.41      0.55      1463\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      5741\n",
      "   macro avg       0.83      0.69      0.72      5741\n",
      "weighted avg       0.83      0.83      0.81      5741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(F\"{ pred_scr_val }, { recall_scr_val }, { f1_scr_val }, { acr_score_val }\")\n",
    "print(classification_report(Y_val_true, Y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/train_evaluation.txt\",\"a\") as file:\n",
    "    file.write(F\" Precision score is -> { pred_scr_val } \\n\")\n",
    "    file.write(F\" Recall score is -> { recall_scr_val } \\n\")\n",
    "    file.write(F\" F1 score is -> { f1_scr_val } \\n\")\n",
    "    file.write(F\" Acuuracy score is -> { acr_score_val } \\n\")\n",
    "    file.write(F\"Classification report is below \\n\\n\\n { classification_report(Y_val_true, Y_val_pred) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8164297921373409, 0.47956761058425623, 0.48051420795849625, 0.82658460073356\n"
     ]
    }
   ],
   "source": [
    "avg_precision = ( pred_scr + pred_scr_val ) / 2\n",
    "avg_recall = ( recall_scr / recall_scr_val ) / 2\n",
    "avg_f1scr = ( f1_scr / f1_scr_val ) / 2\n",
    "avg_accr = ( acr_score + acr_score_val ) / 2\n",
    "\n",
    "print(F\"{ avg_precision }, { avg_recall }, { avg_f1scr }, { avg_accr }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/train_evaluation.txt\",\"a\") as file:\n",
    "    file.write(F\"AVERAGE EVALUATION REPORT -> \\n\\n\\n\\n\")\n",
    "    file.write(F\" Average Precision score is -> { avg_precision } \\n\")\n",
    "    file.write(F\" Average Recall score is -> { avg_recall } \\n\")\n",
    "    file.write(F\" Average F1 score is -> { avg_f1scr } \\n\")\n",
    "    file.write(F\" Average Acuuracy score is -> { avg_accr }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/test_evaluation.txt\",\"a\") as file:\n",
    "    file.write(F\"AVERAGE EVALUATION REPORT -> \\n\\n\\n\\n\")\n",
    "    file.write(F\" Average Precision score is -> { avg_precision } \\n\")\n",
    "    file.write(F\" Average Recall score is -> { avg_recall } \\n\")\n",
    "    file.write(F\" Average F1 score is -> { avg_f1scr } \\n\")\n",
    "    file.write(F\" Average Acuuracy score is -> { avg_accr }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
